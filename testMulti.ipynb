{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b6f39b-b3f7-445b-8a1f-879a9c9ddeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"multi key test\") \\\n",
    "  .master(\"local[16]\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc= spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470e5182-f821-47ff-af80-8de6b86ffbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000000005000000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of the first 100 Whole numbers\n",
    "rdd = sc.parallelize(range(10000000000 +1))\n",
    "rdd.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4b128f-0da3-4e94-8a95-e2b1e0082333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.9</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.04932</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472</td>\n",
       "      <td>6.849</td>\n",
       "      <td>70.3</td>\n",
       "      <td>3.1827</td>\n",
       "      <td>7.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>396.9</td>\n",
       "      <td>7.53</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>24.39380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>4.652</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.9</td>\n",
       "      <td>28.28</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.17142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448</td>\n",
       "      <td>5.682</td>\n",
       "      <td>33.8</td>\n",
       "      <td>5.1004</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>396.9</td>\n",
       "      <td>10.21</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.30347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493</td>\n",
       "      <td>6.312</td>\n",
       "      <td>28.9</td>\n",
       "      <td>5.4159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>396.9</td>\n",
       "      <td>6.15</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "307   0.04932  33.0   2.18   0.0  0.472  6.849   70.3  3.1827   7.0  222.0   \n",
       "386  24.39380   0.0  18.10   0.0  0.700  4.652  100.0  1.4672  24.0  666.0   \n",
       "45    0.17142   0.0   6.91   0.0  0.448  5.682   33.8  5.1004   3.0  233.0   \n",
       "326   0.30347   0.0   7.38   0.0  0.493  6.312   28.9  5.4159   5.0  287.0   \n",
       "\n",
       "     PTRATIO      B  LSTAT  target  \n",
       "505     21.0  396.9   7.88    11.9  \n",
       "307     18.4  396.9   7.53    28.2  \n",
       "386     20.2  396.9  28.28    10.5  \n",
       "45      17.9  396.9  10.21    19.3  \n",
       "326     19.6  396.9   6.15    23.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the boston data set\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# convert to a Pandas Data Frame\n",
    "boston_pd = pd.DataFrame(data= np.c_[boston['data'],boston['target']], \n",
    "              columns= np.append(boston['feature_names'], 'target')).sample(frac=1)\n",
    "print(boston_pd.shape)\n",
    "boston_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41b2a91-ad05-42fa-b06d-7011f1349140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7230573095749625\n",
      "MAE: 3.6288318442391945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# split into data and label arrays \n",
    "y = boston_pd['target']\n",
    "X = boston_pd.drop(['target'], axis=1)\n",
    "\n",
    "# create training (~80%) and test data sets\n",
    "X_train = X[:400]\n",
    "X_test = X[400:]\n",
    "y_train = y[:400]\n",
    "y_test = y[400:]\n",
    "\n",
    "# train a classifier \n",
    "lr = LinearRegression()\n",
    "model = lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# error metrics\n",
    "r = pearsonr(y_pred, y_test)\n",
    "mae = sum(abs(y_pred - y_test))/len(y_test)\n",
    "print(\"R-sqaured: \" + str(r[0]**2))\n",
    "print(\"MAE: \" + str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926aa011-1eb9-4cf5-8785-26c1562683ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRIM=0.04741, ZN=0.0, INDUS=11.93, CHAS=0.0, NOX=0.573, RM=6.03, AGE=80.8, DIS=2.505, RAD=1.0, TAX=273.0, PTRATIO=21.0, B=396.9, LSTAT=7.88, target=11.9),\n",
       " Row(CRIM=0.04932, ZN=33.0, INDUS=2.18, CHAS=0.0, NOX=0.472, RM=6.849, AGE=70.3, DIS=3.1827, RAD=7.0, TAX=222.0, PTRATIO=18.4, B=396.9, LSTAT=7.53, target=28.2),\n",
       " Row(CRIM=24.3938, ZN=0.0, INDUS=18.1, CHAS=0.0, NOX=0.7, RM=4.652, AGE=100.0, DIS=1.4672, RAD=24.0, TAX=666.0, PTRATIO=20.2, B=396.9, LSTAT=28.28, target=10.5),\n",
       " Row(CRIM=0.17142, ZN=0.0, INDUS=6.91, CHAS=0.0, NOX=0.448, RM=5.682, AGE=33.8, DIS=5.1004, RAD=3.0, TAX=233.0, PTRATIO=17.9, B=396.9, LSTAT=10.21, target=19.3),\n",
       " Row(CRIM=0.30347, ZN=0.0, INDUS=7.38, CHAS=0.0, NOX=0.493, RM=6.312, AGE=28.9, DIS=5.4159, RAD=5.0, TAX=287.0, PTRATIO=19.6, B=396.9, LSTAT=6.15, target=23.0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0474, 0.0, 11.93, 0.0, 0.573, 6.03, 80.8, 2.505, 1.0, 273.0, 21.0, 396.9, 7.88]), target=11.9),\n",
       " Row(features=DenseVector([0.0493, 33.0, 2.18, 0.0, 0.472, 6.849, 70.3, 3.1827, 7.0, 222.0, 18.4, 396.9, 7.53]), target=28.2),\n",
       " Row(features=DenseVector([24.3938, 0.0, 18.1, 0.0, 0.7, 4.652, 100.0, 1.4672, 24.0, 666.0, 20.2, 396.9, 28.28]), target=10.5),\n",
       " Row(features=DenseVector([0.1714, 0.0, 6.91, 0.0, 0.448, 5.682, 33.8, 5.1004, 3.0, 233.0, 17.9, 396.9, 10.21]), target=19.3),\n",
       " Row(features=DenseVector([0.3035, 0.0, 7.38, 0.0, 0.493, 6.312, 28.9, 5.4159, 5.0, 287.0, 19.6, 396.9, 6.15]), target=23.0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to a Spark data frame\n",
    "boston_sp = spark.createDataFrame(boston_pd)\n",
    "display(boston_sp.take(5))\n",
    "\n",
    "# split into training and test spark data frames\n",
    "boston_train = spark.createDataFrame(boston_pd[:400])\n",
    "boston_test = spark.createDataFrame(boston_pd[400:])\n",
    "\n",
    "# convert to vector representation for MLlib\n",
    "assembler = VectorAssembler(inputCols= boston_train.schema.names[:(boston_pd.shape[1] - 1)],  \n",
    "                                                                        outputCol=\"features\" )\n",
    "boston_train = assembler.transform(boston_train).select('features', 'target') \n",
    "boston_test = assembler.transform(boston_test).select('features', 'target') \n",
    "\n",
    "display(boston_train.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd53da6a-f397-4a58-9dad-e0573990744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7115010821501254\n"
     ]
    }
   ],
   "source": [
    "# linear regresion with Spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# linear regression \n",
    "lr = LinearRegression(maxIter=10, regParam=0.1, \n",
    "                      elasticNetParam=0.5, labelCol=\"target\")\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(boston_train)\n",
    "boston_pred = model.transform(boston_test)\n",
    "\n",
    "# calculate results \n",
    "r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "print(\"R-sqaured: \" + str(r**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853db355-9259-4950-85a7-54fafc22ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaured: 0.7230573095749673\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "crossval = CrossValidator(estimator=LinearRegression(labelCol = \"target\"),  \n",
    "                         estimatorParamMaps=ParamGridBuilder().addGrid(\n",
    "                           LinearRegression.elasticNetParam, [0, 0.5, 1.0]).build(),\n",
    "                         evaluator=RegressionEvaluator(\n",
    "                           labelCol = \"target\", metricName = \"r2\"),\n",
    "                         numFolds=10)\n",
    "\n",
    "# cross validate the model and select the best fit\n",
    "cvModel = crossval.fit(boston_train) \n",
    "model = cvModel.bestModel\n",
    "\n",
    "# calculate results \n",
    "boston_pred = model.transform(boston_test)\n",
    "r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "print(\"R-sqaured: \" + str(r**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a1f9e5-5d0d-47de-91d2-436c1b125945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 0.8539412099834258], [20, 0.8533219084658166], [50, 0.8720090447050126]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn version \n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# allow up to 5 concurrent threads\n",
    "pool = ThreadPool(5)\n",
    "\n",
    "# hyperparameters to test out (n_trees)\n",
    "parameters = [ 10, 20, 50]\n",
    "\n",
    "# define a function to train a RF model and return metrics \n",
    "def sklearn_random_forest(trees, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # train a random forest regressor with the specified number of trees\n",
    "    rf= RFR(n_estimators = trees)\n",
    "    model = rf.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    r = pearsonr(y_pred, y_test)\n",
    "\n",
    "    # return the number of trees, and the R value \n",
    "    return [trees, r[0]**2]  \n",
    "\n",
    "# run the tasks \n",
    "pool.map(lambda trees: sklearn_random_forest(trees, X_train,\n",
    "                                           X_test, y_train, y_test), parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f683b2-f234-4153-910e-0cef56605794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 0.8436836892201441], [20, 0.8911474300093474], [50, 0.8769789726473258]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark version\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# define a function to train a RF model and return metrics \n",
    "def mllib_random_forest(trees, boston_train, boston_test):\n",
    "\n",
    "    # train a random forest regressor with the specified number of trees\n",
    "    rf = RandomForestRegressor(numTrees = trees, labelCol=\"target\")\n",
    "    model = rf.fit(boston_train)\n",
    "\n",
    "    # make predictions\n",
    "    boston_pred = model.transform(boston_test)\n",
    "    r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "\n",
    "    # return the number of trees, and the R value \n",
    "    return [trees, r**2]\n",
    "  \n",
    "# run the tasks \n",
    "pool.map(lambda trees: mllib_random_forest(trees, boston_train, boston_test), parameters)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14d115ec-219a-4121-aee4-537798b32173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|trees|         r_squared|\n",
      "+-----+------------------+\n",
      "|   11|0.8426812014785933|\n",
      "|   20|0.8553074478093554|\n",
      "|   50|0.8650469974108973|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58162)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# setup the spark data frame as a table\n",
    "boston_sp.createOrReplaceTempView(\"boston\")\n",
    "\n",
    "# add train/test label and expand the data set by 3x (each num trees parameter)\n",
    "full_df = spark.sql(\"\"\"\n",
    "  select *\n",
    "  from (\n",
    "    select *, case when rand() < 0.8 then 1 else 0 end as training \n",
    "    from boston\n",
    "  ) b\n",
    "  cross join (\n",
    "      select 11 as trees union all select 20 as trees union all select 50 as trees)\n",
    "\"\"\")\n",
    "\n",
    "schema = StructType([StructField('trees', LongType(), True),\n",
    "                     StructField('r_squared', DoubleType(), True)])  \n",
    "\n",
    "#@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def train_RF(boston_pd):\n",
    "    trees = boston_pd['trees'].unique()[0]\n",
    "\n",
    "    # get the train and test groups \n",
    "    boston_train = boston_pd[boston_pd['training'] == 1]\n",
    "    boston_test = boston_pd[boston_pd['training'] == 0] \n",
    "        \n",
    "    # create data and label groups \n",
    "    y_train = boston_train['target']\n",
    "    X_train = boston_train.drop(['target'], axis=1)\n",
    "    y_test = boston_test['target']\n",
    "    X_test = boston_test.drop(['target'], axis=1)\n",
    "   \n",
    "    # train a classifier \n",
    "    rf= RFR(n_estimators = trees)\n",
    "    model = rf.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    r = pearsonr(y_pred, y_test)\n",
    "    \n",
    "    # return the number of trees, and the R value \n",
    "    return pd.DataFrame({'trees': trees, 'r_squared': (r[0]**2)}, index=[0])\n",
    "  \n",
    "# use the Pandas UDF\n",
    "# results = full_df.groupby('trees').apply(train_RF)\n",
    "\n",
    "# print the results \n",
    "# print(results.take(3))\n",
    "\n",
    "full_df.groupby('trees').applyInPandas(\n",
    "    train_RF, schema=\"trees long, r_squared double\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cb2b262-5471-4ba0-a908-1948663a6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  v|\n",
      "+---+-------------------+\n",
      "|  1|-0.7071067811865475|\n",
      "|  1| 0.7071067811865475|\n",
      "|  2|-0.8320502943378437|\n",
      "|  2|-0.2773500981126146|\n",
      "|  2| 1.1094003924504583|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from pyspark.sql.functions import pandas_udf, ceil\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").applyInPandas(\n",
    "    normalize, schema=\"id long, v double\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b5e3d-468b-4ae7-92fa-c87daf819725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
